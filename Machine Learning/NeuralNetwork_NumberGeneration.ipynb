{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f562bab0",
   "metadata": {},
   "source": [
    "# Neural Network for Happiness Score Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3132f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec07690c2ef4fcaa92d6327de803a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('2', '3', '4',\n",
    "           '5', '6', '7', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1259ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Convolution Neural Network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5829ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss Function Generation\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4b4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.236\n",
      "[1,  4000] loss: 1.950\n",
      "[1,  6000] loss: 1.732\n",
      "[1,  8000] loss: 1.626\n",
      "[1, 10000] loss: 1.553\n",
      "[1, 12000] loss: 1.493\n",
      "[2,  2000] loss: 1.443\n",
      "[2,  4000] loss: 1.391\n",
      "[2,  6000] loss: 1.364\n",
      "[2,  8000] loss: 1.342\n",
      "[2, 10000] loss: 1.309\n",
      "[2, 12000] loss: 1.277\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training the Network\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cbcd28",
   "metadata": {},
   "source": [
    "Here we have to save the trained Neural Network to device, which is CUDA in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727f72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH ='C:/Users/reddy/Documents/GitHub/Datathon/Machine Learning/happinesstraining_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47f50a",
   "metadata": {},
   "source": [
    "The next steps after training are to test the data given certain types that do not currently have Happiness scores. The way we do this is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f054c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndataiter = iter(testloader)\\nimages, labels = next(dataiter)\\n\\n# print images\\nimshow(torchvision.utils.make_grid(images))\\nprint('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338c185",
   "metadata": {},
   "source": [
    "The reason I surrounded code in comments is because we currently do not have the test data (countries with GDP, Population, Life Expectancy, and Size of Country. When given more time, we can web-scape to get more data and put it into this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
